{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10411554,"sourceType":"datasetVersion","datasetId":6452379},{"sourceId":10525138,"sourceType":"datasetVersion","datasetId":6514064}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:08:28.884074Z","iopub.execute_input":"2025-05-10T12:08:28.884334Z","iopub.status.idle":"2025-05-10T12:08:34.306912Z","shell.execute_reply.started":"2025-05-10T12:08:28.884309Z","shell.execute_reply":"2025-05-10T12:08:34.306108Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\nimport torch\n\n# Use BERT model for embeddings\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbert_model = AutoModel.from_pretrained(model_name)\n\n# Check for device availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move the BERT model to the device\nbert_model.to(device)\nbert_model.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:08:34.307668Z","iopub.execute_input":"2025-05-10T12:08:34.308156Z","iopub.status.idle":"2025-05-10T12:08:38.805076Z","shell.execute_reply.started":"2025-05-10T12:08:34.308133Z","shell.execute_reply":"2025-05-10T12:08:38.804353Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5209a46f420440892a06ea54b1b09aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9020271abeaf4af1927a8e1d17aaa48c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8a0661bdfbd48e5a93ab35597f9e1d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"108b2f3ada82465a81339d69a4b9ec68"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49bd0b97c12441d1be7d1eab289308c4"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def featurize_smiles(smiles_list):\n    features = []\n    with torch.no_grad():\n        for smiles in smiles_list:\n            encoded = tokenizer(smiles, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n            output = bert_model(**encoded)\n            features.append(output.pooler_output.squeeze(0))\n\n    features_tensor = torch.stack(features)\n    return features_tensor.to(dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:08:38.805895Z","iopub.execute_input":"2025-05-10T12:08:38.806301Z","iopub.status.idle":"2025-05-10T12:08:38.810679Z","shell.execute_reply.started":"2025-05-10T12:08:38.806277Z","shell.execute_reply":"2025-05-10T12:08:38.809977Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/data-entire/entire_data.xlsx')  \nsmiles = df[\"SMILES_1\"].values\ntarget = df[\"gap\"].values \n\nfeatures = featurize_smiles(smiles)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T12:08:38.811513Z","iopub.execute_input":"2025-05-10T12:08:38.811785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n\nclass PredictionModel(nn.Module):\n    def __init__(self, input_dim):\n        super(PredictionModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ninput_dim = features.shape[1] \nmodel = PredictionModel(input_dim).to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_tensor = TensorDataset(torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(y_train, dtype=torch.float32).to(device))\nval_tensor = TensorDataset(torch.tensor(X_val, dtype=torch.float32).to(device), torch.tensor(y_val, dtype=torch.float32).to(device))\ntest_tensor = TensorDataset(torch.tensor(X_test, dtype=torch.float32).to(device), torch.tensor(y_test, dtype=torch.float32).to(device))\n\nbatch_size = 32\ntrain_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_tensor, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_tensor, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 100\nconsecutive_increase_counter = 0\nmax_allowed_increases = 4\nprev_val_loss = float('inf')\n\ntrain_losses = []\nval_losses = []\n\nsave_path = \"/kaggle/working/BERT-77M-MTR_80_best_model.pth\"\n\ncount = 0\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    count += 1\n    model.train()\n    train_loss = 0\n    for batch_X, batch_y in train_loader:\n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs.squeeze(), batch_y)\n\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n\n    avg_train_loss = train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch_X, batch_y in val_loader:\n            outputs = model(batch_X)\n            loss = criterion(outputs.squeeze(), batch_y)\n            val_loss += loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n\n    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n\n    if avg_val_loss > prev_val_loss:\n        consecutive_increase_counter += 1\n    else:\n        consecutive_increase_counter = 0 \n        \n    if consecutive_increase_counter >= max_allowed_increases:\n        print(f\"Early stopping triggered after {max_allowed_increases} consecutive increases in validation loss!\")\n        break\n        \n    prev_val_loss = avg_val_loss\n    torch.save(model.state_dict(), save_path) \n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint(f\"Training time: {training_time} seconds\")\nprint(f\"Training time per epoch: {training_time / count} seconds\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(train_losses[1:], label='Train Loss')\nplt.plot(val_losses[1:], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\nactual_values = []\npredicted_values = []\n\nwith torch.no_grad():\n    for images, properties in test_loader:\n        images, properties = images.to(device), properties.to(device)\n\n        outputs = model(images).squeeze(-1) \n        actual_values.extend(properties.cpu().numpy())\n        predicted_values.extend(outputs.cpu().numpy())\n\nactual_values = np.array(actual_values)\npredicted_values = np.array(predicted_values)\n\nmse = mean_squared_error(actual_values, predicted_values)\nmae = mean_absolute_error(actual_values, predicted_values)\nr2 = r2_score(actual_values, predicted_values)\n\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"R-Squared (RÂ²): {r2:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.scatter(actual_values, predicted_values, alpha=0.6, label=\"Predictions\")\nplt.plot(\n    [min(actual_values), max(actual_values)],\n    [min(actual_values), max(actual_values)],\n    color=\"red\",\n    linestyle=\"--\",\n    label=\"Ideal Fit\"\n)\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Predicted Values\")\nplt.title(\"Actual vs Predicted\")\nplt.legend()\nplt.grid()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}